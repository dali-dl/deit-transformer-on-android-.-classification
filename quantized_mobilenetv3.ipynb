{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models.mobilenetv2 import _make_divisible, ConvBNActivation\n",
    "\n",
    "\n",
    "__all__ = [\"MobileNetV3\", \"mobilenet_v3_large\", \"mobilenet_v3_small\"]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"mobilenet_v3_large\": \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\",\n",
    "    \"mobilenet_v3_small\": \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    # Implemented as described at Figure 4 of the MobileNetV3 paper\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: Tensor, inplace: bool) -> Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return F.hardsigmoid(scale, inplace=inplace)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "\n",
    "\n",
    "class InvertedResidualConfig:\n",
    "    # Stores information listed at Tables 1 and 2 of the MobileNetV3 paper\n",
    "    def __init__(self, input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool,\n",
    "                 activation: str, stride: int, dilation: int, width_mult: float):\n",
    "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        self.kernel = kernel\n",
    "        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n",
    "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        self.use_se = use_se\n",
    "        self.use_hs = activation == \"HS\"\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_mult: float):\n",
    "        return _make_divisible(channels * width_mult, 8)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    # Implemented as described at section 5 of MobileNetV3 paper\n",
    "    def __init__(self, cnf: InvertedResidualConfig, norm_layer: Callable[..., nn.Module],\n",
    "                 se_layer: Callable[..., nn.Module] = SqueezeExcitation):\n",
    "        super().__init__()\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError('illegal stride value')\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.Hardswish if cnf.use_hs else nn.ReLU\n",
    "\n",
    "        # expand\n",
    "        if cnf.expanded_channels != cnf.input_channels:\n",
    "            layers.append(ConvBNActivation(cnf.input_channels, cnf.expanded_channels, kernel_size=1,\n",
    "                                           norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "\n",
    "        # depthwise\n",
    "        stride = 1 if cnf.dilation > 1 else cnf.stride\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.expanded_channels, kernel_size=cnf.kernel,\n",
    "                                       stride=stride, dilation=cnf.dilation, groups=cnf.expanded_channels,\n",
    "                                       norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "        if cnf.use_se:\n",
    "            layers.append(se_layer(cnf.expanded_channels))\n",
    "\n",
    "        # project\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Identity))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.out_channels = cnf.out_channels\n",
    "        self._is_cn = cnf.stride > 1\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result += input\n",
    "        return result\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            inverted_residual_setting: List[InvertedResidualConfig],\n",
    "            last_channel: int,\n",
    "            num_classes: int = 1000,\n",
    "            block: Optional[Callable[..., nn.Module]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "            **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        MobileNet V3 main class\n",
    "        Args:\n",
    "            inverted_residual_setting (List[InvertedResidualConfig]): Network structure\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "            num_classes (int): Number of classes\n",
    "            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not inverted_residual_setting:\n",
    "            raise ValueError(\"The inverted_residual_setting should not be empty\")\n",
    "        elif not (isinstance(inverted_residual_setting, Sequence) and\n",
    "                  all([isinstance(s, InvertedResidualConfig) for s in inverted_residual_setting])):\n",
    "            raise TypeError(\"The inverted_residual_setting should be List[InvertedResidualConfig]\")\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        # building first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(ConvBNActivation(3, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Hardswish))\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for cnf in inverted_residual_setting:\n",
    "            layers.append(block(cnf, norm_layer))\n",
    "\n",
    "        # building last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = 6 * lastconv_input_channels\n",
    "        layers.append(ConvBNActivation(lastconv_input_channels, lastconv_output_channels, kernel_size=1,\n",
    "                                       norm_layer=norm_layer, activation_layer=nn.Hardswish))\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lastconv_output_channels, last_channel),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor, x_supp: Tensor, threshold: float) -> Tensor:\n",
    "        # get the features of the query and support images\n",
    "        query = self._forward_impl(x)\n",
    "        support = self._forward_impl(x_supp)\n",
    "        \n",
    "        # compute the distance between the support and query images\n",
    "        distance = torch.sum((query - support)**2, dim=-1)\n",
    "        probs = torch.softmax(-distance, dim=0)\n",
    "        \n",
    "        # prob is not smaller than the threshold or not?\n",
    "        if torch.max(probs) < threshold:\n",
    "            return torch.tensor(data=-111)\n",
    "        \n",
    "        return torch.argmax(probs)\n",
    "\n",
    "def _mobilenet_v3_conf(arch: str, width_mult: float = 1.0, reduced_tail: bool = False, dilated: bool = False,\n",
    "                       **kwargs: Any):\n",
    "    reduce_divider = 2 if reduced_tail else 1\n",
    "    dilation = 2 if dilated else 1\n",
    "\n",
    "    bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n",
    "    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_mult=width_mult)\n",
    "\n",
    "    if arch == \"mobilenet_v3_large\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, False, \"RE\", 1, 1),\n",
    "            bneck_conf(16, 3, 64, 24, False, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(24, 3, 72, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 72, 40, True, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 3, 240, 80, False, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(80, 3, 200, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 480, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 3, 672, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 5, 672, 160 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1280 // reduce_divider)  # C5\n",
    "    elif arch == \"mobilenet_v3_small\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 288, 96 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1024 // reduce_divider)  # C5\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type {}\".format(arch))\n",
    "\n",
    "    return inverted_residual_setting, last_channel\n",
    "\n",
    "\n",
    "def _mobilenet_v3_model(\n",
    "    arch: str,\n",
    "    inverted_residual_setting: List[InvertedResidualConfig],\n",
    "    last_channel: int,\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    "):\n",
    "    model = MobileNetV3(inverted_residual_setting, last_channel, **kwargs)\n",
    "    if pretrained:\n",
    "        if model_urls.get(arch, None) is None:\n",
    "            raise ValueError(\"No checkpoint is available for model type {}\".format(arch))\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_large(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a large MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_large\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, **kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def mobilenet_v3_small(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a small MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_small\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, **kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "model_tiny = mobilenet_v3_large(pretrained=True)\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "ts_model = torch.jit.script(model_tiny)\n",
    "optimized_torchscript_model = optimize_for_mobile(ts_model)\n",
    "optimized_torchscript_model.save(\"mobilenetv3_large.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}